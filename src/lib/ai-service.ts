/**
 * AI æœåŠ¡é€‰æ‹©å™¨
 * æ”¯æŒæ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹© Gemini æˆ– DeepSeek API
 */

import { callDeepSeekAPI } from './deepseek';
import { callGeminiAPI } from './gemini';

export type AIModel = 'gemini' | 'deepseek';

export interface AIConfig {
  model?: AIModel;
  temperature?: number;
  maxTokens?: number;
  timeout?: number;
}

/**
 * è·å–å½“å‰é…ç½®çš„ AI æ¨¡å‹
 */
export function getCurrentAIModel(): AIModel {
  const modelType = process.env.AI_MODEL as AIModel;

  // æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
  if (modelType === 'gemini' || modelType === 'deepseek') {
    return modelType;
  }

  // è‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœæœ‰ Gemini API Keyï¼Œä¼˜å…ˆä½¿ç”¨ Gemini
  if (process.env.GOOGLE_API_KEY) {
    return 'gemini';
  }

  // é»˜è®¤ä½¿ç”¨ DeepSeek
  return 'deepseek';
}

/**
 * æ£€æŸ¥ AI æ¨¡å‹æ˜¯å¦å¯ç”¨
 */
export function isAIModelAvailable(model: AIModel): boolean {
  switch (model) {
    case 'gemini':
      return !!process.env.GOOGLE_API_KEY;
    case 'deepseek':
      return !!process.env.DEEPSEEK_API_KEY;
    default:
      return false;
  }
}

/**
 * è·å–å¯ç”¨çš„ AI æ¨¡å‹åˆ—è¡¨
 */
export function getAvailableAIModels(): AIModel[] {
  const models: AIModel[] = [];

  if (isAIModelAvailable('gemini')) {
    models.push('gemini');
  }

  if (isAIModelAvailable('deepseek')) {
    models.push('deepseek');
  }

  return models;
}

/**
 * è·å–é™çº§ AI æ¨¡å‹
 */
export function getFallbackAIModel(primaryModel: AIModel): AIModel | null {
  const availableModels = getAvailableAIModels();

  // è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹ï¼ˆæ’é™¤ä¸»æ¨¡å‹ï¼‰
  for (const model of availableModels) {
    if (model !== primaryModel) {
      return model;
    }
  }

  return null;
}

/**
 * ç»Ÿä¸€çš„ AI API è°ƒç”¨æ¥å£
 */
export async function callAI(
  prompt: string,
  config: AIConfig = {}
): Promise<string> {
  // ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹
  const selectedModel = config.model || getCurrentAIModel();

  console.log(`ğŸ¯ é€‰æ‹© AI æ¨¡å‹: ${selectedModel}`);
  console.log(`ğŸ“‹ å¯ç”¨æ¨¡å‹: ${getAvailableAIModels().join(', ')}`);

  // æ£€æŸ¥æ‰€é€‰æ¨¡å‹æ˜¯å¦å¯ç”¨
  if (!isAIModelAvailable(selectedModel)) {
    console.warn(`âš ï¸ ${selectedModel} æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•é™çº§`);

    const fallbackModel = getFallbackAIModel(selectedModel);
    if (fallbackModel) {
      console.log(`ğŸ”„ é™çº§åˆ°: ${fallbackModel}`);
      return callAIWithModel(prompt, fallbackModel, config);
    }

    throw new Error(`æ²¡æœ‰å¯ç”¨çš„ AI æ¨¡å‹ã€‚è¯·æ£€æŸ¥ API å¯†é’¥é…ç½®ã€‚`);
  }

  try {
    return await callAIWithModel(prompt, selectedModel, config);
  } catch (error) {
    console.error(`âŒ ${selectedModel} è°ƒç”¨å¤±è´¥:`, error);

    // å°è¯•é™çº§
    const fallbackModel = getFallbackAIModel(selectedModel);
    if (fallbackModel) {
      console.log(`ğŸ”„ ${selectedModel} å¤±è´¥ï¼Œé™çº§åˆ°: ${fallbackModel}`);
      try {
        return await callAIWithModel(prompt, fallbackModel, config);
      } catch (fallbackError) {
        console.error(`âŒ é™çº§æ¨¡å‹ ${fallbackModel} ä¹Ÿå¤±è´¥äº†:`, fallbackError);
      }
    }

    // æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
    throw error;
  }
}

/**
 * ä½¿ç”¨æŒ‡å®šæ¨¡å‹è°ƒç”¨ AI
 */
async function callAIWithModel(
  prompt: string,
  model: AIModel,
  config: AIConfig
): Promise<string> {
  switch (model) {
    case 'gemini':
      return await callGeminiAPI(prompt, {
        temperature: config.temperature,
        maxTokens: config.maxTokens,
        timeout: config.timeout
      });

    case 'deepseek':
      return await callDeepSeekAPI(prompt, {
        temperature: config.temperature,
        max_tokens: config.maxTokens,
        timeout: config.timeout
      });

    default:
      throw new Error(`ä¸æ”¯æŒçš„ AI æ¨¡å‹: ${model}`);
  }
}

/**
 * æ£€æµ‹éƒ¨ç½²ç¯å¢ƒ
 */
export function detectEnvironment(): 'vercel' | 'huawei' | 'local' {
  if (process.env.VERCEL === '1') {
    return 'vercel';
  }

  // å¯ä»¥é€šè¿‡å…¶ä»–ç¯å¢ƒå˜é‡è¯†åˆ«åä¸ºäº‘
  if (process.env.HUAWEI_CLOUD === '1' || process.env.NODE_ENV === 'production' && !process.env.VERCEL) {
    return 'huawei';
  }

  return 'local';
}

/**
 * æ£€æµ‹æ˜¯å¦åº”è¯¥ä½¿ç”¨ç¼“å­˜
 */
export function shouldUseCache(): boolean {
  const environment = detectEnvironment();

  // Vercel ç¯å¢ƒä¸ä½¿ç”¨ç¼“å­˜ï¼ˆæ— çŠ¶æ€ï¼‰
  if (environment === 'vercel') {
    return false;
  }

  // æœ¬åœ°å’Œåä¸ºäº‘ç¯å¢ƒä½¿ç”¨ç¼“å­˜
  return true;
}

/**
 * è·å– AI æœåŠ¡çŠ¶æ€ä¿¡æ¯
 */
export function getAIServiceStatus() {
  const environment = detectEnvironment();
  const currentModel = getCurrentAIModel();
  const availableModels = getAvailableAIModels();
  const useCache = shouldUseCache();

  return {
    environment,
    currentModel,
    availableModels,
    useCache,
    hasGeminiKey: isAIModelAvailable('gemini'),
    hasDeepSeekKey: isAIModelAvailable('deepseek')
  };
}